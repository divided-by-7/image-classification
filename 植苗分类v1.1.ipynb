{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c9a26d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d749eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e310f1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "all_files = []\n",
    "for label, directory in enumerate(os.listdir(file_path)):\n",
    "#     print(label) # 0-11\n",
    "#     print(directory) # 12个文件夹的名字\n",
    "    mapping[label] = directory # 为mapping字典添加内容\n",
    "    tmp_list = [[file_path + '/' + directory+'/'+file,label] for file in os.listdir(file_path + '/' + directory)] # 将每个文件夹里的文件路径和类别写成list\n",
    "    all_files.extend(tmp_list) # 将这些list添加到一起，得到所有图片的路径和对应的label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37b1948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping字典里的内容 {0: 'Black-grass', 1: 'Charlock', 2: 'Cleavers', 3: 'Common Chickweed', 4: 'Common wheat', 5: 'Fat Hen', 6: 'Loose Silky-bent', 7: 'Maize', 8: 'Scentless Mayweed'， 9: 'Shepherds Purse',10: 'Small-flowered Cranesbill',11: 'Sugar beet'}\n",
    "# mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a897eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files列表的内容[['train/Black-grass/0050f38b3.png', 0],['train/Black-grass/0183fdf68.png', 0], ['train/Black-grass/0260cffa8.png', 0],\n",
    "# all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b84de08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_files) # 将all_files的顺序打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e640ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打乱后的all_files [['train/Black-grass/d622ca3d2.png', 0], ['train/Maize/a5c2eec2d.png', 7], ['train/Scentless Mayweed/4205830a0.png', 8], ['train/Loose Silky-bent/b3f997421.png', 6], ['train/Black-grass/0183fdf68.png', 0], ['train/Shepherds Purse/eae41be4f.png', 9],\n",
    "# all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081c5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_target = 200 # 由于图片大小不同 指定为固定尺寸200*200\n",
    "batch_size = 32\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.transforms.Resize((resize_target,resize_target)),\n",
    "        transforms.transforms.ColorJitter(brightness=1, contrast=0.5, saturation=0.5, hue=0),\n",
    "        transforms.transforms.RandomHorizontalFlip(),\n",
    "        transforms.transforms.RandomRotation(180),\n",
    "        transforms.transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8688554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_colors(img):\n",
    "    img = transforms.functional.adjust_brightness(img, 2)\n",
    "    img = transforms.functional.adjust_contrast(img, 1.1)\n",
    "    img = transforms.functional.adjust_saturation(img, 1.1)\n",
    "    return img\n",
    "\n",
    "\n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads through a DB one by one, perform transforms\"\"\"\n",
    "    def __init__(self, file_list, segment = False, transform = False):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform \n",
    "        self.segment = segment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.file_list[idx][0]\n",
    "        label = self.file_list[idx][1]\n",
    "        img = Image.open(item).convert('RGB')\n",
    "        if self.segment:\n",
    "            img = segment_plant(img)\n",
    "        img = adjust_colors(img)\n",
    "        if self.transform:\n",
    "            img = self.transform (img)\n",
    "        else:\n",
    "            img =  transforms.functional.resize(img, (resize_target,resize_target))\n",
    "            img =  transforms.functional.to_tensor(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba38f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset(all_files, segment = False,transform = data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f33258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4750"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca533cb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx= 0\n",
      "torch.Size([3, 200, 200])\n",
      "label= 3\n",
      "idx= 1\n",
      "torch.Size([3, 200, 200])\n",
      "label= 1\n",
      "idx= 2\n",
      "torch.Size([3, 200, 200])\n",
      "label= 9\n",
      "idx= 3\n",
      "torch.Size([3, 200, 200])\n",
      "label= 9\n",
      "idx= 4\n",
      "torch.Size([3, 200, 200])\n",
      "label= 3\n",
      "idx= 5\n",
      "torch.Size([3, 200, 200])\n",
      "label= 8\n"
     ]
    }
   ],
   "source": [
    "# 检查dataset\n",
    "for idx,(img,label) in enumerate(dataset):\n",
    "    print('idx=',idx)\n",
    "#     print(img)\n",
    "    print(img.shape)\n",
    "    print('label=',label)\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc52ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(np.floor(0.2 * len(dataset))) # 设置val所占比例0.2\n",
    "indices = list(range(len(dataset)))\n",
    "train_idx, valid_idx = indices[split:], indices[:split] \n",
    "train_sampler_random = torch.utils.data.SubsetRandomSampler(train_idx) \n",
    "valid_sampler_random = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=train_sampler_random)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "893ccefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx= 0\n",
      "torch.Size([32, 3, 200, 200])\n",
      "label= tensor([ 8,  3,  4,  6, 10, 10,  8,  6,  3,  4,  5, 10,  8,  5,  2,  9,  3,  5,\n",
      "        10,  0, 10,  6, 10,  2,  6,  5,  6, 11, 10,  9,  9,  2])\n",
      "batch_idx= 1\n",
      "torch.Size([32, 3, 200, 200])\n",
      "label= tensor([ 3,  8,  6,  9,  3, 10,  7,  2,  3,  5, 10,  0,  3, 10,  9,  8,  5,  3,\n",
      "         3,  2, 11,  1,  9,  7,  3,  1,  3,  8,  1,  6,  5,  2])\n",
      "batch_idx= 2\n",
      "torch.Size([32, 3, 200, 200])\n",
      "label= tensor([ 0,  9,  3,  6, 10,  4, 10,  5,  8,  8,  4,  2,  6,  8,  9,  8,  0,  6,\n",
      "         5, 11,  6,  4,  0,  2,  6,  9, 10,  3, 11,  6,  6,  2])\n"
     ]
    }
   ],
   "source": [
    "# 检查train_loader\n",
    "for batch_idx,(img,label) in enumerate(train_loader):\n",
    "    print('batch_idx=',batch_idx)\n",
    "#     print(img)\n",
    "    print(img.shape)\n",
    "    print('label=',label)\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b349a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx= 0\n",
      "torch.Size([32, 3, 200, 200])\n",
      "label= tensor([10,  2, 11,  4,  2,  4, 11,  3,  3,  4, 11, 10,  7,  9,  6,  2,  8,  2,\n",
      "         5,  9, 10,  8, 11, 10, 10,  4,  5,  1,  8,  9,  8,  4])\n",
      "batch_idx= 1\n",
      "torch.Size([32, 3, 200, 200])\n",
      "label= tensor([ 5, 10,  6,  8,  1,  4, 10,  3,  8,  6,  6,  1,  3,  2,  1,  5,  8,  8,\n",
      "         8,  3,  4, 11,  6,  3,  0,  5,  4,  4, 10,  6,  5, 10])\n",
      "batch_idx= 2\n",
      "torch.Size([32, 3, 200, 200])\n",
      "label= tensor([11,  3,  0, 11, 11,  2,  3,  8, 10,  7,  6, 11,  1,  5, 10,  6,  5,  5,\n",
      "         5, 11,  6,  7,  7,  2,  7,  5,  1,  8,  6, 10,  1,  8])\n"
     ]
    }
   ],
   "source": [
    "# 检查val_loader\n",
    "for batch_idx,(img,label) in enumerate(val_loader):\n",
    "    print('batch_idx=',batch_idx)\n",
    "#     print(img)\n",
    "    print(img.shape)\n",
    "    print('label=',label)\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7422514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1cec4ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2054d642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'------------------------------模型构建(这里为方便测试先使用torchvision自带的模型及ImageNet上的预训练模型)------------------------------'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-'*30+'模型构建(这里为方便测试先使用torchvision自带的模型及ImageNet上的预训练模型)'+'-'*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7c04d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "resnet18.fc = torch.nn.Linear(512,12)\n",
    "vgg11 = torchvision.models.vgg11(pretrained=True)\n",
    "vgg11.classifier[6] = torch.nn.Linear(4096,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96730df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0'if torch.cuda.is_available() else 'cpu') # 选择设备\n",
    "print('device:',device)\n",
    "Model = resnet18 # 选择模型\n",
    "model = Model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss() # 损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9) # 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a71f133a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'------------------------------开始训练------------------------------'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-'*30+'开始训练'+'-'*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "957a316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 14952), started 0:00:24 ago. (Use '!kill 14952' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1a1fb251aed1aac8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1a1fb251aed1aac8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!rm -rf logs\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs/fit --port=6007\n",
    "# --logdir后面为tensorboard数据的地址  port为端口号\n",
    "writer = SummaryWriter(log_dir='logs/fit/loss') # tensorboard 绘图\n",
    "writer1 = SummaryWriter(log_dir='logs/fit/acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2952c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28807a40",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------epoch = 1------------------------\n",
      "**epoch= 1 **train_loss= 2.6431334018707275 **acc= 0.21875 **batch / num of batch  =   10 / 119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11832/3541159772.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtrain_acc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiko\\pycharmprojects\\yolov5-5.0\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiko\\pycharmprojects\\yolov5-5.0\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiko\\pycharmprojects\\yolov5-5.0\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiko\\pycharmprojects\\yolov5-5.0\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11832/3625019997.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment_plant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjust_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11832/3625019997.py\u001b[0m in \u001b[0;36madjust_colors\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiko\\pycharmprojects\\yolov5-5.0\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36madjust_saturation\u001b[1;34m(img, saturation_factor)\u001b[0m\n\u001b[0;32m    778\u001b[0m     \"\"\"\n\u001b[0;32m    779\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiko\\pycharmprojects\\yolov5-5.0\\venv\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36madjust_saturation\u001b[1;34m(img, saturation_factor)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0menhancer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menhancer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menhance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaturation_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiko\\pycharmprojects\\yolov5-5.0\\venv\\lib\\site-packages\\PIL\\ImageEnhance.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"LA\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegenerate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiko\\pycharmprojects\\yolov5-5.0\\venv\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdither\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 100\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    start1 = time.time()\n",
    "    print('----------------------epoch = %s------------------------' % epoch)\n",
    "    total_step = 0\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    for ind, (img, cls) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        x, y = img.to(device), cls.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        _,predicted = torch.max(y_pred.data,1)\n",
    "        acc = (predicted == y).sum()/len(y)\n",
    "        train_loss_list.append(loss.item())\n",
    "        train_acc_list.append(acc.item())\n",
    "        total_step += 1\n",
    "        if total_step % 10 == 0:\n",
    "            print('**epoch=', epoch, '**train_loss=', loss.item(),'**acc=',acc.item(), '**batch / num of batch  =  ', total_step,'/',len(train_loader))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_valid_loss = 0\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    with torch.no_grad():\n",
    "        total_step1 = 0\n",
    "        for ind, (img, cls) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            x1, y1 = img.to(device), cls.to(device)\n",
    "            y_pred1 = model(x1)\n",
    "            loss = criterion(y_pred1, y1)  # 每次for循环得到一个batch的loss\n",
    "\n",
    "            _,predicted = torch.max(y_pred1.data,1)\n",
    "            acc = (predicted == y1).sum()/len(y1)\n",
    "\n",
    "            valid_loss_list.append(loss.item())\n",
    "            valid_acc_list.append(acc.item())\n",
    "            \n",
    "            total_step1 += 1\n",
    "            if total_step1 % 10 == 0:\n",
    "                print('**epoch=', epoch, '**valid_loss=', loss.item(),'**valid_acc=',acc.item(), '**step / num of batch  =  ', total_step1,'/',len(test_loader))\n",
    "    train_loss = np.mean(train_loss_list)            \n",
    "    valid_loss = np.mean(valid_loss_list)\n",
    "    train_acc =np.mean(train_acc_list)\n",
    "    valid_acc =np.mean(valid_acc_list)\n",
    "\n",
    "    if valid_acc>= best_acc:\n",
    "      best_acc = valid_acc\n",
    "      torch.save(model.state_dict(),'model_state_dict_epoch.pt')\n",
    "      print('已保存第%s个epoch的模型' % epoch)\n",
    "    print('epoch=', epoch, 'mean_train_loss=', train_loss)\n",
    "    print('epoch=', epoch, 'mean_train_acc=', train_acc)\n",
    "    print('epoch=', epoch, 'mean_valid_loss=', valid_loss)\n",
    "    print('epoch=', epoch, 'mean_valid_acc=', valid_acc)\n",
    "    final_train_loss.append(train_loss)\n",
    "    final_train_acc.append(train_acc)\n",
    "    final_valid_loss.append(valid_loss)\n",
    "    final_valid_acc.append(valid_acc)\n",
    "    writer.add_scalar('train_loss',train_loss,epoch) # 把loss值写入summary writer\n",
    "    writer.add_scalar('valid_loss',valid_loss,epoch)\n",
    "    writer1.add_scalar('train_acc',train_acc,epoch)\n",
    "    writer1.add_scalar('valid_acc',valid_acc,epoch) \n",
    "    print('本epoch运行时长%s' % (time.time()-start1))\n",
    "end = time.time()\n",
    "print('运行时长%s' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Times New Roman')\n",
    "\n",
    "epochs = range(num_epochs)\n",
    "plt.plot(epochs, final_train_acc, 'r', label='Training Acc')\n",
    "plt.plot(epochs, final_valid_acc, 'b',linewidth=1, label='Validation Acc') \n",
    "ax = plt.subplot(111)\n",
    "# 设置刻度字体大小\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "# 设置坐标标签字体大小\n",
    "ax.set_xlabel('Number of iteration', fontsize=18)\n",
    "ax.set_ylabel('Acc', fontsize=18)\n",
    "ax.legend(fontsize=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
